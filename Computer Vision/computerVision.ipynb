{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6909bf55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060 Laptop GPU'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b00c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96390bae",
   "metadata": {},
   "source": [
    "# Image assigning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec00083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "DATA_DIR = 'Images'\n",
    "\n",
    "data_class2id_map = {\"Classic\": 0, \"Modern\": 1, \"Soviet\": 2 }\n",
    "data_id2class_map = {0: \"Classic\", 1: \"Modern\", 2: \"Soviet\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6a842c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classic\n",
      "_______________________\n",
      "Modern\n",
      "_______________________\n",
      "Soviet\n",
      "_______________________\n"
     ]
    }
   ],
   "source": [
    "for label_path in os.listdir(DATA_DIR):\n",
    "    if 'train' in  label_path or 'test' in label_path:\n",
    "        continue\n",
    "        \n",
    "    print(label_path)\n",
    "    label_id = data_class2id_map[label_path]\n",
    "    image_type_path = os.path.join(DATA_DIR,label_path)\n",
    "    for image_path in os.listdir(image_type_path):\n",
    "        image_id = uuid.uuid4().int\n",
    "        image_name = f\"{image_id}_{label_id}.jpg\"\n",
    "        os.rename(os.path.join(image_type_path, image_path), os.path.join(image_type_path,image_name))\n",
    "    print(\"_______________________\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "167d7f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Images\\\\train'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(DATA_DIR, 'train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cc544fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.makedirs(os.path.join(DATA_DIR, 'train'), exist_ok=True)\n",
    "# os.makedirs(os.path.join(DATA_DIR, 'test'), exist_ok=True)\n",
    "\n",
    "[os.makedirs(os.path.join(DATA_DIR, 'train', folder), exist_ok=True) for \\\n",
    " folder in list(data_class2id_map.keys())]\n",
    "\n",
    "[os.makedirs(os.path.join(DATA_DIR, 'test', folder), exist_ok=True) for \\\n",
    " folder in list(data_class2id_map.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27a4ecbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classic\n",
      "Modern\n",
      "Soviet\n"
     ]
    }
   ],
   "source": [
    "split_ratio = 0.1\n",
    "testing_paths = []\n",
    "training_paths = []\n",
    "\n",
    "for label_path in os.listdir(DATA_DIR):\n",
    "    if 'train' in  label_path or 'test' in label_path:\n",
    "        continue\n",
    "    else:\n",
    "        print(label_path)\n",
    "        \n",
    "        sub_path = os.path.join(DATA_DIR, label_path)\n",
    "        \n",
    "        label_id = data_class2id_map[label_path]\n",
    "        image_type_path = os.path.join(DATA_DIR,label_path)\n",
    "        files =  os.listdir(image_type_path)\n",
    "        num_images = len(files)\n",
    "        test_images = list(np.random.choice(files, size=int(num_images*split_ratio), replace=False))\n",
    "        train_images = list(set(files) - set(test_images))\n",
    "\n",
    "        assert sorted(train_images + test_images) == sorted(files)\n",
    "        \n",
    "        testing_paths += [os.path.join(sub_path, t_i) for t_i in test_images]\n",
    "        training_paths += [os.path.join(sub_path, t_i) for t_i in train_images]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e09cc189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Images\\\\Classic\\\\255544330452439562989672129852919410354_0.jpg',\n",
       " 'Images\\\\Classic\\\\338535707135202624929950440699785428028_0.jpg',\n",
       " 'Images\\\\Classic\\\\184439146238878862816570300395195491390_0.jpg',\n",
       " 'Images\\\\Classic\\\\49828480862642322331870647240833131915_0.jpg',\n",
       " 'Images\\\\Classic\\\\18449471709679300563827624902644967132_0.jpg',\n",
       " 'Images\\\\Classic\\\\330549538701655674635472979142397910022_0.jpg',\n",
       " 'Images\\\\Classic\\\\321006798778387740544992994532564608832_0.jpg',\n",
       " 'Images\\\\Classic\\\\237879402739436274199279759452709042728_0.jpg',\n",
       " 'Images\\\\Classic\\\\152123482479402919544422840030829390368_0.jpg',\n",
       " 'Images\\\\Classic\\\\81728332479680054921153204900040668847_0.jpg',\n",
       " 'Images\\\\Classic\\\\244550273952312124856564411256054240392_0.jpg',\n",
       " 'Images\\\\Classic\\\\243031332650110769401591779706839048932_0.jpg',\n",
       " 'Images\\\\Classic\\\\164882053139644828348451784927518659454_0.jpg',\n",
       " 'Images\\\\Classic\\\\221722630344525691119727171575240990560_0.jpg',\n",
       " 'Images\\\\Classic\\\\124799694547932158787519655212524363196_0.jpg',\n",
       " 'Images\\\\Classic\\\\81977941042393819890834098360654443810_0.jpg',\n",
       " 'Images\\\\Classic\\\\321233575145025520266795147969334553145_0.jpg',\n",
       " 'Images\\\\Classic\\\\63786110215030978569947640835190973185_0.jpg',\n",
       " 'Images\\\\Classic\\\\59173132156227194474380324441437359563_0.jpg',\n",
       " 'Images\\\\Classic\\\\291885275963516699883936693209146371721_0.jpg',\n",
       " 'Images\\\\Classic\\\\5443269301311948147811974906214671234_0.jpg',\n",
       " 'Images\\\\Classic\\\\324910879898307607712032735765259056755_0.jpg',\n",
       " 'Images\\\\Classic\\\\274362477606161839038926123827814647650_0.jpg',\n",
       " 'Images\\\\Classic\\\\10411543205517536302345492320187310557_0.jpg',\n",
       " 'Images\\\\Classic\\\\255271961874642966924293133904150125693_0.jpg',\n",
       " 'Images\\\\Classic\\\\10892244560702517445094328000168316281_0.jpg',\n",
       " 'Images\\\\Classic\\\\215303457674052131890215120375563398531_0.jpg',\n",
       " 'Images\\\\Classic\\\\173810897946915443644815443283870432707_0.jpg',\n",
       " 'Images\\\\Classic\\\\57622487227239839237382404515238504565_0.jpg',\n",
       " 'Images\\\\Classic\\\\165646436362122133271526326769868046919_0.jpg',\n",
       " 'Images\\\\Classic\\\\17379522027988418736564839677263489407_0.jpg',\n",
       " 'Images\\\\Classic\\\\160056437825632475041953214033196532249_0.jpg',\n",
       " 'Images\\\\Classic\\\\177483535708053988794151415255636076113_0.jpg',\n",
       " 'Images\\\\Classic\\\\169451707908657786837953732296920876257_0.jpg',\n",
       " 'Images\\\\Classic\\\\192732164974617507240332391422817893478_0.jpg',\n",
       " 'Images\\\\Classic\\\\251059811638903913191651872284784751614_0.jpg',\n",
       " 'Images\\\\Classic\\\\44526035083565463239336072073143759105_0.jpg',\n",
       " 'Images\\\\Classic\\\\184712831444053446434152441527560635111_0.jpg',\n",
       " 'Images\\\\Classic\\\\231203334985558644073756592160718163275_0.jpg',\n",
       " 'Images\\\\Classic\\\\160059454747451147780953059660324128435_0.jpg',\n",
       " 'Images\\\\Classic\\\\174854951779180039673789857299843396877_0.jpg',\n",
       " 'Images\\\\Classic\\\\198867189568910275637920163957640487187_0.jpg',\n",
       " 'Images\\\\Classic\\\\328387918322310304828481081747739572692_0.jpg',\n",
       " 'Images\\\\Classic\\\\140567309674727584410990857115159322137_0.jpg',\n",
       " 'Images\\\\Classic\\\\178515264540977189391406029108536510164_0.jpg',\n",
       " 'Images\\\\Classic\\\\176782390439729913598749898395519455524_0.jpg',\n",
       " 'Images\\\\Classic\\\\272612221076254143067513001064888029438_0.jpg',\n",
       " 'Images\\\\Classic\\\\75988651475865921829460828489991043585_0.jpg',\n",
       " 'Images\\\\Classic\\\\173553739482458072933769545079614234722_0.jpg',\n",
       " 'Images\\\\Classic\\\\213340180568183157257723360602281363557_0.jpg',\n",
       " 'Images\\\\Classic\\\\289113194082692887682111091783404969490_0.jpg',\n",
       " 'Images\\\\Classic\\\\165943539006627367271076915068103190848_0.jpg',\n",
       " 'Images\\\\Classic\\\\30281965499322554902248841246951791548_0.jpg',\n",
       " 'Images\\\\Classic\\\\315990160533837532222156006879422261961_0.jpg',\n",
       " 'Images\\\\Classic\\\\10603186951621146793086812083833108798_0.jpg',\n",
       " 'Images\\\\Classic\\\\14990905988635403219986106366675657497_0.jpg',\n",
       " 'Images\\\\Classic\\\\24442631256459886539482603119954068529_0.jpg',\n",
       " 'Images\\\\Classic\\\\7681049694376092789566784415646095951_0.jpg',\n",
       " 'Images\\\\Classic\\\\193093754581245054880327911167116000302_0.jpg',\n",
       " 'Images\\\\Classic\\\\257430717207748030280256109827980007050_0.jpg',\n",
       " 'Images\\\\Classic\\\\29324293551261467189561005864133799461_0.jpg',\n",
       " 'Images\\\\Classic\\\\207794899975566551669857826032508790911_0.jpg',\n",
       " 'Images\\\\Classic\\\\336424942603346435969390369337181783047_0.jpg',\n",
       " 'Images\\\\Classic\\\\101450266018769423373171616853988179709_0.jpg',\n",
       " 'Images\\\\Classic\\\\216991043866287151741536712753446070436_0.jpg',\n",
       " 'Images\\\\Classic\\\\218317789715809343370723690428512688736_0.jpg',\n",
       " 'Images\\\\Classic\\\\214361089240577703720967512915801786271_0.jpg',\n",
       " 'Images\\\\Classic\\\\276514055936610073557189685600268743414_0.jpg',\n",
       " 'Images\\\\Classic\\\\262406391711192880738215571305722813237_0.jpg',\n",
       " 'Images\\\\Classic\\\\324333144659853378458093502573898122907_0.jpg',\n",
       " 'Images\\\\Classic\\\\258960034488412547816933885740890875824_0.jpg',\n",
       " 'Images\\\\Classic\\\\63155461157689203697124645308025718416_0.jpg',\n",
       " 'Images\\\\Classic\\\\248060699199642102847309331296667942827_0.jpg',\n",
       " 'Images\\\\Classic\\\\9739342292235858854876413722534642098_0.jpg',\n",
       " 'Images\\\\Classic\\\\20291218577119777052385034785454328931_0.jpg',\n",
       " 'Images\\\\Modern\\\\292962928148378373823267035253335226281_1.jpg',\n",
       " 'Images\\\\Modern\\\\1131891700391184545011628720674821220_1.jpg',\n",
       " 'Images\\\\Modern\\\\247872989583084713020757758970085050269_1.jpg',\n",
       " 'Images\\\\Modern\\\\160954645846293018438433903569268206009_1.jpg',\n",
       " 'Images\\\\Modern\\\\179243660345122495184612363042227241849_1.jpg',\n",
       " 'Images\\\\Modern\\\\62989748492371798093642305524726059642_1.jpg',\n",
       " 'Images\\\\Modern\\\\78922231279832126222363975047020902355_1.jpg',\n",
       " 'Images\\\\Modern\\\\141441528553978229595424331481586490038_1.jpg',\n",
       " 'Images\\\\Modern\\\\233620676044304525482850367960364831175_1.jpg',\n",
       " 'Images\\\\Modern\\\\311585706865272555953730502538966565698_1.jpg',\n",
       " 'Images\\\\Modern\\\\281206178288883175196533194177154686493_1.jpg',\n",
       " 'Images\\\\Modern\\\\162767451738895887677603887625949691175_1.jpg',\n",
       " 'Images\\\\Modern\\\\92651609541958594010539638880276182087_1.jpg',\n",
       " 'Images\\\\Modern\\\\205788265260169885292072382073872648455_1.jpg',\n",
       " 'Images\\\\Modern\\\\312936982668493034933469986896524960133_1.jpg',\n",
       " 'Images\\\\Modern\\\\31938694013757087431588806979106037456_1.jpg',\n",
       " 'Images\\\\Modern\\\\48623896586203951546458820238406285349_1.jpg',\n",
       " 'Images\\\\Modern\\\\303534132325790682473577985078304195683_1.jpg',\n",
       " 'Images\\\\Modern\\\\127812681016456594442023250573364426946_1.jpg',\n",
       " 'Images\\\\Modern\\\\277406652693329506819723894679893587736_1.jpg',\n",
       " 'Images\\\\Modern\\\\191128756501711597461037142376189076775_1.jpg',\n",
       " 'Images\\\\Modern\\\\33883033568193695241847194280563642531_1.jpg',\n",
       " 'Images\\\\Modern\\\\278076755588638151484277763642013395666_1.jpg',\n",
       " 'Images\\\\Modern\\\\12699875016210955939246060529377758630_1.jpg',\n",
       " 'Images\\\\Modern\\\\275531785854068817592564438124378592683_1.jpg',\n",
       " 'Images\\\\Modern\\\\142326661713785053176681242832589837928_1.jpg',\n",
       " 'Images\\\\Modern\\\\318713493524730867971992263733157196700_1.jpg',\n",
       " 'Images\\\\Modern\\\\76039765169916923335047711799123625123_1.jpg',\n",
       " 'Images\\\\Modern\\\\268910216275425107541402693085228329093_1.jpg',\n",
       " 'Images\\\\Modern\\\\238760551809062597620813545941641612614_1.jpg',\n",
       " 'Images\\\\Modern\\\\146631587042648131327341973398637404066_1.jpg',\n",
       " 'Images\\\\Modern\\\\102307101101144583077174228472248800444_1.jpg',\n",
       " 'Images\\\\Modern\\\\191097503706861636957368153383572199988_1.jpg',\n",
       " 'Images\\\\Modern\\\\337910847646869419683261918458571022541_1.jpg',\n",
       " 'Images\\\\Modern\\\\153687690762293546611212043648850862347_1.jpg',\n",
       " 'Images\\\\Modern\\\\293358624556030832465028855027546601131_1.jpg',\n",
       " 'Images\\\\Modern\\\\275312465678138829157032044047761103895_1.jpg',\n",
       " 'Images\\\\Modern\\\\307639718246237136728480531148213662035_1.jpg',\n",
       " 'Images\\\\Modern\\\\172956870874392977300922677725367761121_1.jpg',\n",
       " 'Images\\\\Modern\\\\258833894063944749016777338342701370375_1.jpg',\n",
       " 'Images\\\\Modern\\\\99385270856304235973092009817163948352_1.jpg',\n",
       " 'Images\\\\Modern\\\\151805041965790064950182629713288479149_1.jpg',\n",
       " 'Images\\\\Modern\\\\219199129008454169109261286091975595758_1.jpg',\n",
       " 'Images\\\\Modern\\\\127325044390021336740522078486127765076_1.jpg',\n",
       " 'Images\\\\Modern\\\\262624663715816811921429708880314808618_1.jpg',\n",
       " 'Images\\\\Modern\\\\84068876055783134759674961337427074538_1.jpg',\n",
       " 'Images\\\\Modern\\\\21219723370531791067169209193713136191_1.jpg',\n",
       " 'Images\\\\Modern\\\\188702082043003457934840624995817226872_1.jpg',\n",
       " 'Images\\\\Modern\\\\89692919507214722397340350548898089249_1.jpg',\n",
       " 'Images\\\\Modern\\\\330865554661858381412489812844021532759_1.jpg',\n",
       " 'Images\\\\Modern\\\\135123770473626751844264776399226956839_1.jpg',\n",
       " 'Images\\\\Modern\\\\272409236799685117513094665030423905046_1.jpg',\n",
       " 'Images\\\\Modern\\\\199097814565588076979341406034705498799_1.jpg',\n",
       " 'Images\\\\Modern\\\\272911168350772295933338551902241507845_1.jpg',\n",
       " 'Images\\\\Modern\\\\76734114698875703514677058736298624654_1.jpg',\n",
       " 'Images\\\\Modern\\\\336558192360408326448017510753993860773_1.jpg',\n",
       " 'Images\\\\Modern\\\\285799268578211510743035702990554262930_1.jpg',\n",
       " 'Images\\\\Modern\\\\24892946948950981507105890518115567317_1.jpg',\n",
       " 'Images\\\\Modern\\\\4193286482240786205652689307681814402_1.jpg',\n",
       " 'Images\\\\Modern\\\\202259868255924473773808963232447874417_1.jpg',\n",
       " 'Images\\\\Modern\\\\126839517097186913803461201878337438270_1.jpg',\n",
       " 'Images\\\\Modern\\\\142527389587321131449280154806062512843_1.jpg',\n",
       " 'Images\\\\Modern\\\\282301369611359833062518772744465619851_1.jpg',\n",
       " 'Images\\\\Modern\\\\14472428706011061103142213413669880085_1.jpg',\n",
       " 'Images\\\\Modern\\\\93355624203050126191407673420837828316_1.jpg',\n",
       " 'Images\\\\Modern\\\\26369947552595679240074674108927926941_1.jpg',\n",
       " 'Images\\\\Modern\\\\219246534995186597203209188779573137286_1.jpg',\n",
       " 'Images\\\\Modern\\\\238279623051034516179794228359557065472_1.jpg',\n",
       " 'Images\\\\Modern\\\\28271503267254254497172336742544973969_1.jpg',\n",
       " 'Images\\\\Modern\\\\112524989089348781487669492254837488839_1.jpg',\n",
       " 'Images\\\\Modern\\\\122305118415823292095946998487496569561_1.jpg',\n",
       " 'Images\\\\Modern\\\\228013193191032682239405674455844710948_1.jpg',\n",
       " 'Images\\\\Modern\\\\55879188515110601659970179403566519292_1.jpg',\n",
       " 'Images\\\\Modern\\\\261333857027311999786219200178942593530_1.jpg',\n",
       " 'Images\\\\Modern\\\\18174212816384800016166272996715046964_1.jpg',\n",
       " 'Images\\\\Modern\\\\248047017398842796253212208155407079246_1.jpg',\n",
       " 'Images\\\\Soviet\\\\3156812511806672659774246219365864314_2.jpg',\n",
       " 'Images\\\\Soviet\\\\227556975787049702166416573810896766658_2.jpg',\n",
       " 'Images\\\\Soviet\\\\86711485898771839972606687684588764289_2.jpg',\n",
       " 'Images\\\\Soviet\\\\90857889113398793765819404914130551739_2.jpg',\n",
       " 'Images\\\\Soviet\\\\49498221924157455396883259395330924828_2.jpg',\n",
       " 'Images\\\\Soviet\\\\21758064466897209998105626750177473588_2.jpg',\n",
       " 'Images\\\\Soviet\\\\199336086676543044254263964772116435745_2.jpg',\n",
       " 'Images\\\\Soviet\\\\319107521747013596258397004528631338073_2.jpg',\n",
       " 'Images\\\\Soviet\\\\16938428132014588279776601727341381271_2.jpg',\n",
       " 'Images\\\\Soviet\\\\334025672462646966472370123033788174093_2.jpg',\n",
       " 'Images\\\\Soviet\\\\68514186304244584109025973073485155400_2.jpg',\n",
       " 'Images\\\\Soviet\\\\327927724870853467019317510761039553246_2.jpg',\n",
       " 'Images\\\\Soviet\\\\104191065384658387629637848781603463901_2.jpg',\n",
       " 'Images\\\\Soviet\\\\109069126243110076195751828069452080385_2.jpg',\n",
       " 'Images\\\\Soviet\\\\67644552952335132877890582706650986464_2.jpg',\n",
       " 'Images\\\\Soviet\\\\223303607359124412739626446709996506536_2.jpg',\n",
       " 'Images\\\\Soviet\\\\6204325922861566050870527445060884179_2.jpg',\n",
       " 'Images\\\\Soviet\\\\80277738852512525372221735559735221505_2.jpg',\n",
       " 'Images\\\\Soviet\\\\314597121721859666843888672350200673939_2.jpg',\n",
       " 'Images\\\\Soviet\\\\7237945619561395713236112788893181883_2.jpg',\n",
       " 'Images\\\\Soviet\\\\31737867335875575996451997410249645037_2.jpg',\n",
       " 'Images\\\\Soviet\\\\256464180639100276573988341469080991055_2.jpg',\n",
       " 'Images\\\\Soviet\\\\317764129261010648306258296427045002464_2.jpg',\n",
       " 'Images\\\\Soviet\\\\218528325569921719721879554343003073021_2.jpg',\n",
       " 'Images\\\\Soviet\\\\329425971565573388726234748872485657054_2.jpg',\n",
       " 'Images\\\\Soviet\\\\7800543856129855551081989343297354028_2.jpg',\n",
       " 'Images\\\\Soviet\\\\288920904216654944458945683910330698940_2.jpg',\n",
       " 'Images\\\\Soviet\\\\185125166811257313826613453201762693449_2.jpg',\n",
       " 'Images\\\\Soviet\\\\264090791896283924475208382382647537729_2.jpg',\n",
       " 'Images\\\\Soviet\\\\260984338321512411755955142975965816800_2.jpg',\n",
       " 'Images\\\\Soviet\\\\98070275069976705581909198803104924468_2.jpg',\n",
       " 'Images\\\\Soviet\\\\38008317891277664704509801278385140386_2.jpg',\n",
       " 'Images\\\\Soviet\\\\77495221896631329686103020148128401847_2.jpg',\n",
       " 'Images\\\\Soviet\\\\158082702146453567795138624098295465301_2.jpg',\n",
       " 'Images\\\\Soviet\\\\121666214676251982328097696921702184455_2.jpg',\n",
       " 'Images\\\\Soviet\\\\215841188580696744437660219182524333090_2.jpg',\n",
       " 'Images\\\\Soviet\\\\326368551546531477926926184625413885062_2.jpg',\n",
       " 'Images\\\\Soviet\\\\327107558434709491577622810031871997884_2.jpg',\n",
       " 'Images\\\\Soviet\\\\204545286961134787023268231530024930385_2.jpg',\n",
       " 'Images\\\\Soviet\\\\231291922465887616711702743268011373760_2.jpg',\n",
       " 'Images\\\\Soviet\\\\9665385237439571725435259921446072059_2.jpg',\n",
       " 'Images\\\\Soviet\\\\232382514565113272532118500428359530042_2.jpg',\n",
       " 'Images\\\\Soviet\\\\8513939581219104193236900817319187125_2.jpg',\n",
       " 'Images\\\\Soviet\\\\162882587336110958942806764870984220889_2.jpg',\n",
       " 'Images\\\\Soviet\\\\30090012095688052358187386073351611701_2.jpg',\n",
       " 'Images\\\\Soviet\\\\329920455668170484975621876216656283735_2.jpg',\n",
       " 'Images\\\\Soviet\\\\132506956384671989158017953187436823061_2.jpg',\n",
       " 'Images\\\\Soviet\\\\261996239544304132993294259510931855698_2.jpg',\n",
       " 'Images\\\\Soviet\\\\321637462504484915439987778691398076417_2.jpg',\n",
       " 'Images\\\\Soviet\\\\208022752928613023743357904788714511487_2.jpg',\n",
       " 'Images\\\\Soviet\\\\309358417565489908930350409485438325738_2.jpg',\n",
       " 'Images\\\\Soviet\\\\50237442238775031112238105477222645279_2.jpg',\n",
       " 'Images\\\\Soviet\\\\77178215778858965747295314884197161341_2.jpg',\n",
       " 'Images\\\\Soviet\\\\332902558570279839425697088599795736242_2.jpg',\n",
       " 'Images\\\\Soviet\\\\16727495356192522134920853617080469257_2.jpg',\n",
       " 'Images\\\\Soviet\\\\187394001154496601144677548162195040731_2.jpg',\n",
       " 'Images\\\\Soviet\\\\91855898627074500068603359609350890334_2.jpg',\n",
       " 'Images\\\\Soviet\\\\202874112163304994146613774365488711101_2.jpg',\n",
       " 'Images\\\\Soviet\\\\27348014987009960420068319149157676147_2.jpg',\n",
       " 'Images\\\\Soviet\\\\239914874671733413864915097203626477558_2.jpg',\n",
       " 'Images\\\\Soviet\\\\225213243719447957581817248845491843522_2.jpg',\n",
       " 'Images\\\\Soviet\\\\69392985270808106917985962728667087673_2.jpg',\n",
       " 'Images\\\\Soviet\\\\296724859344607352565707790182558354401_2.jpg',\n",
       " 'Images\\\\Soviet\\\\211396768930626268625516694941269359549_2.jpg',\n",
       " 'Images\\\\Soviet\\\\67733429125300662355889057527366212497_2.jpg',\n",
       " 'Images\\\\Soviet\\\\100644456911051400334170630721403294959_2.jpg',\n",
       " 'Images\\\\Soviet\\\\333454863836413861971410329829230398678_2.jpg',\n",
       " 'Images\\\\Soviet\\\\334493126608231370760784680845479999738_2.jpg',\n",
       " 'Images\\\\Soviet\\\\151379464540563025360685342785686320971_2.jpg',\n",
       " 'Images\\\\Soviet\\\\263512886713683461827624192734460154541_2.jpg',\n",
       " 'Images\\\\Soviet\\\\189636356132712526445047937717107908253_2.jpg',\n",
       " 'Images\\\\Soviet\\\\285307293022329042182606863844512942125_2.jpg',\n",
       " 'Images\\\\Soviet\\\\232541005725554526067275849709120186922_2.jpg',\n",
       " 'Images\\\\Soviet\\\\192614320653782733874799872902821328706_2.jpg',\n",
       " 'Images\\\\Soviet\\\\320335132467646726861032335954599176799_2.jpg',\n",
       " 'Images\\\\Soviet\\\\5612298313566516686588811578598226614_2.jpg']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fad38079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5612298313566516686588811578598226614_2.jpg'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(training_paths[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5529b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from typing import List, NoReturn\n",
    "\n",
    "def train_test_split_paths(data_paths: List = [], split_type:str = \"train\") -> NoReturn:\n",
    "    for data_path in data_paths:\n",
    "        output_path = os.path.join(DATA_DIR, split_type)\n",
    "        data_label_folder = data_id2class_map[int(data_path.split('_')[-1][0])]\n",
    "        output_path = os.path.join(output_path, data_label_folder)\n",
    "        output_path = os.path.join(output_path, os.path.basename(data_path))\n",
    "        \n",
    "        shutil.copyfile(data_path, output_path)\n",
    "        \n",
    "train_test_split_paths(training_paths, 'train')\n",
    "train_test_split_paths(testing_paths, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9321a31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'0.jpg': 8, '1.jpg': 8, '2.jpg': 8})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "labels_train = [i.split('_')[-1] for i in training_paths]\n",
    "labels_test = [i.split('_')[-1] for i in testing_paths]\n",
    "\n",
    "Counter(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2e3792",
   "metadata": {},
   "source": [
    "# Torch Data pipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2c20b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.11.0\n",
      "Torchvision Version:  0.12.0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os.path as osp\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "import time\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3d12342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = DATA_DIR\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"squeezenet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 3\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 15\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72c47cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'test':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2c62142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67a81b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (3): Fire(\n",
      "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Fire(\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Fire(\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (7): Fire(\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Fire(\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Fire(\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Fire(\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (12): Fire(\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Conv2d(512, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "021b7ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'test']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'test']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2834dd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(params_to_update, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1254c9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\anaconda3\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0011 Acc: 0.5198\n",
      "test Loss: 0.8243 Acc: 0.6944\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.7124 Acc: 0.7048\n",
      "test Loss: 0.6479 Acc: 0.7500\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.5731 Acc: 0.7812\n",
      "test Loss: 0.5384 Acc: 0.8056\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.5370 Acc: 0.7974\n",
      "test Loss: 0.6093 Acc: 0.7778\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.4457 Acc: 0.8399\n",
      "test Loss: 0.4952 Acc: 0.8472\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.4094 Acc: 0.8370\n",
      "test Loss: 0.3899 Acc: 0.8750\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.3872 Acc: 0.8664\n",
      "test Loss: 0.4425 Acc: 0.8472\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.4119 Acc: 0.8341\n",
      "test Loss: 0.3351 Acc: 0.8194\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.3958 Acc: 0.8620\n",
      "test Loss: 0.2936 Acc: 0.8472\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.3944 Acc: 0.8443\n",
      "test Loss: 0.2520 Acc: 0.9167\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.3445 Acc: 0.8737\n",
      "test Loss: 0.2742 Acc: 0.9167\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.3708 Acc: 0.8634\n",
      "test Loss: 0.2616 Acc: 0.9167\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.3413 Acc: 0.8781\n",
      "test Loss: 0.2487 Acc: 0.9167\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.3330 Acc: 0.8869\n",
      "test Loss: 0.2084 Acc: 0.9306\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.3276 Acc: 0.8899\n",
      "test Loss: 0.2901 Acc: 0.8333\n",
      "\n",
      "Training complete in 1m 32s\n",
      "Best val Acc: 0.930556\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
